\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\input{packages}
\input{commands}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{xx}{2018}{xx-xx}{9/18}{xx/xx}{gabava18}{Guillaume Gautier, R\'emi Bardenet, and Michal Valko}

% Short headings should be running head and authors last names

\ShortHeadings{\DPPy}{Gautier, Bardenet, and Valko}
\firstpageno{1}

\begin{document}

\title{\DPPy: Sampling DPPs with Python}

\author{\name Guillaume Gautier$^{\dagger*}$ \email g.gautier@inria.fr \\
       \name R\'emi Bardenet$^\dagger$ \email remi.bardenet@gmail.com \\
       \name Michal Valko$^{*\ddag}$ \email valkom@google.com\\
       \addr $^\dagger$Univ.\,Lille, CNRS, Centrale Lille, UMR 9189 - CRIStAL, 59651 Villeneuve d'Ascq, France\\
       \addr $^*$INRIA Lille-Nord Europe, 40, avenue Halley 59650, Villeneuve d'Ascq, France. \addr $^\ddag$DeepMind Paris
}

\editor{}

\maketitle

\setcounter{footnote}{3}
\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
    Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning.
    Sampling from DPPs is a challenge and therefore we present \DPPy, a Python toolbox that gathers known exact and approximate sampling algorithms.
    The project is hosted on GitHub\!\footnote{\label{fn:github}\href{https://github.com/guilgautier/DPPy}{\textsf{github.com/guilgautier/DPPy}}}and equipped with an extensive documentation.
    This documentation\!\footnote{\label{fn:docs}\href{https://dppy.readthedocs.io}{\textsf{dppy.readthedocs.io}}}takes the form of a short survey of DPPs and relates each mathematical property with \DPPy\ objects.
\end{abstract}

\begin{keywords}%
    determinantal point processes,
    sampling,
    MCMC,
    random matrices,
    Python
\end{keywords}

\section{Introduction} % (fold)
\label{sec:introduction}

    Determinantal point processes (DPPs) are distributions over configurations of points that encode diversity through a kernel function $K$.
    They were introduced by \citet{Mac75} as models for beams of fermions, and they have since found applications in fields as diverse as probability \citep{Sos00, Kon05, HKPV06}, statistical physics \citep{PaBe11}, Monte Carlo methods \citep{BaHa16}, spatial statistics \citep{LaMoRu15}, and machine learning \citep[ML,][]{KuTa12}.

    In ML, DPPs mainly serve to model diverse sets of items, as in recommendation \citep{KaDeKo16, GaPaKo16} or text summarization \citep{DuBa18}.
    Consequently, MLers  use mostly finite DPPs, which are distributions over subsets of a finite \emph{ground set} of cardinality $M$, parametrized by an $M\times M$ kernel matrix $\bfK$.
    Routine inference tasks such as normalization, marginalization, or sampling have complexity $\calO(M^3)$ \citep{Gil14}.
    For large $M$, $\calO(M^3)$ is a bottleneck, as for other kernel methods, see also \citet{TrBaAm18} for a survey on exact sampling.
    Efficient approximate samplers have thus been developed, ranging from kernel approximation \citep{AKFT13} to MCMC samplers \citep{AnGhRe16, LiJeSr16c, GaBaVa17}.

    In terms of software, the R library \textsf{spatstat}\ \citep{BaTu05}, a general-purpose toolbox on spatial point processes, includes sampling and learning of continuous DPPs with stationary kernels, as described by \citet{LaMoRu15}.
    On the other hand, we propose \DPPy, a turnkey Python implementation of known general algorithms to sample \emph{finite} DPPs.
    We also provide a few algorithms for non-stationary continuous DPPs that are related to random projections and random covariance matrices, which can be of interest for MLers.
    The \DPPy\ project is currently being used by the community (see, e.g., \citealp{BuRaWi19,Kam18,Pou19,DeCaVa19,GaBaVa19})
    \setcounter{footnote}{3}

    \DPPy\ is hosted on GitHub,\!\footnoteref{fn:github}we use \setcounter{footnote}{5}Travis\!\footnote{\href{https://travis-ci.com/guilgautier/DPPy}{\textsf{travis-ci.com/guilgautier/DPPy}}}for continuous integration, Coveralls\!\footnote{\href{https://coveralls.io/github/guilgautier/DPPy}{\textsf{coveralls.io/github/guilgautier/DPPy}}}for test coverage.
    Through ReadTheDocs\hspace{1pt}\footnoteref{fn:docs}we provide an extensive documentation which covers the essential mathematical background and showcases the key properties of DPPs through \DPPy\ objects and associated methods.
    \DPPy\ thus also serves as a tutorial.
     Along the paper, words in magenta point to the documentation.
    % In \Secref{sec:definitions}, we introduce DPPs.
    % In \Secref{sec:sampling} we briefly review the challenging task of sampling

% section introduction (end)

% \section{Determinantal point processes} % (fold)
% \label{sec:determinantal_point_processes}

%     We introduce DPPs and the main sampling algorithm; see \citet{HKPV06} for details.

    \section{Definitions} % (fold)
    \label{sec:definitions}

        A point process $\calX$ on $\bbX$ is a random subset of points $\lrcb{X_1, \dots, X_N} \subset \bbX$, where the number of points $N$ is itself random.
        We further add to the definition that $N$ should be almost surely finite and that all points in a sample are distinct.
        Given a reference measure $\mu$ on~$\bbX$, a point process is usually characterized by its $k$-correlation function $\rho_k$ for all $k$, where
        \begin{equation*}
        \label{eq:correlation_function_intuition}
            \Proba{
                \begin{tabular}{c}
                    $\exists$ one point of the process in\\
                    each ball $B(x_i, \diff x_i), \forall i=1,\dots, k $
                \end{tabular}
            }
            = \rho_k\lrp{x_1,\dots,x_k}
                \prod_{i=1}^k \mu(\diff x_i),
        \end{equation*}
        see \citet[Section 4]{MoWa04}.
        The functions $\rho_k$ describe the interaction among points in $\calX$ by quantifying co-occurrence of points at a set of locations.
        % Considering $\mu$ as the reference measure, the $k$-correlation functions are defined as
        % \begin{equation}
        % \label{eq:k-correlation_function}
        %   \Expe{ \sum_{
        %     \substack{
        %       (X_1,\dots,X_k) \\
        %       X_1 \neq \dots \neq X_k} }
        %     f(X_1,\dots,X_k)
        %     }
        %     = \int_{\bbX^k}
        %       f(x_1,\dots,x_k) \rho_k(x_1,\dots,x_k)
        %       \prod_{i=1}^k \mu(dx_i),
        % \end{equation}

        % for all bounded measurable functions $f:\bbX^k\to \bbC$ and the sum ranges over $k$-tuples of distinct points of $\calX$.

        A point process $\calX$ on $(\bbX,\mu)$ parametrized by a kernel $K:\bbX\times \bbX\rightarrow \bbC$ is said to be \emph{determinantal}, denoted as $\calX\sim\DPP(K)$, if its $k$-correlation functions satisfy
        \begin{equation*}
            \label{eq:k-correlation_function_DPP}
            \rho_k(x_1,\dots,x_k)
              = \det \lrb{K(x_i, x_j)}_{i,j=1}^k,
            \quad \forall k\geq 1.
        \end{equation*}
        In ML, most DPPs are taken from the finite setting where $\bbX = \lrcb{1,\dots,M}$ and $\mu=\sum_{i=1}^M \delta_i$.
        In this context, the kernel function becomes an $M\times M$ matrix $\bfK$, and the correlation functions refer to inclusion probabilities.~DPPs are thus often defined as $\calX\sim \DPP(\bfK)$ if
        \begin{equation}
        \label{eq:inclusion_proba_finite}
            \Proba{S \subset \calX} = \det \bfK_S,
                \quad\forall S\subset \bbX,
        \end{equation}
        where ${\bfK}_S$ denotes the submatrix of $\bfK$ formed by the rows and columns indexed by $S$.
        The kernel matrix $\bfK$ is commonly assumed real-symmetric, in which case the existence and uniqueness of the DPP in \Eqref{eq:inclusion_proba_finite} is equivalent to the condition that its eigenvalues lie in $[0,1]$.
        The result also holds for general Hermitian kernel functions $K$ with additional assumptions \cite[Theorem 3]{Sos00}.
        We note that there are also DPPs with nonsymmetric kernels \citep{BoDiFu10}.

        Oftentimes, ML practitioners favor a more flexible definition of the DPP in terms of a \emph{likelihood kernel} $\bfL$
        ($\Proba{S \subset \calX}\propto \det \bfL_S$) which only requires $\bfL\succeq 0$
        rather than a \emph{correlation kernel} $0\preceq \bfK \preceq I$.
        Yet, the $\bfL$ parametrization makes \Eqref{eq:inclusion_proba_finite} less interpretable and does not cover important cases such as fixed size DPPs which are achievable using projection $\bfK$ kernels.
        To counter the latter issue, \citet[Section 5]{KuTa12} introduced $k$-DPPs which can be understood as DPPs parametrized by a likelihood kernel, conditioned to have exactly $k$ elements.
        However, $k$-DPPs are not DPPs in general.

        The main interest in DPPs in ML is that they can model diversity while being tractable.
        The entries of $\bfK$ encode a notion of similarity between items and \Eqref{eq:inclusion_proba_finite} entails
        \begin{equation*}
        \label{eq:2point_correlation_function_inclusion_proba_finite_case}
          \Proba{\{i,j\} \subset \calX}
            = {\bfK}_{ii}{\bfK}_{jj}-{\bfK}_{ij}{\bfK}_{ji}
            = \Proba{\{i\} \subset \calX}
              \times \Proba{\{j\} \subset \calX}
                - |{\bfK}_{ij}|^2,
        \end{equation*}
        i.e., compared to independent sampling with the same marginals, the more similar items $i$ and $j$ the less likely they co-occur.

\clearpage
        Most point processes that encode diversity are not tractable, in the sense that efficient algorithms to sample, marginalize, or compute normalization constants are not available.
        DPPs are amenable to these tasks \citep{KuTa12,Gil14}.
        % , with polynomial complexity.

    % section definitions (end)

    \section{Sampling determinantal point processes} % (fold)
    \label{sec:sampling}

        We assume henceforth that $K$ is real-symmetric and satisfies suitable conditions \cite[Theorem 3]{Sos00} so that its spectral decomposition is available
        \begin{equation*}
        \label{eq:eigdec_kernel}
        K(x,y)
        \triangleq
          \suml_{i=1}^{\infty}
            \lambda_i \phi_i(x) \phi_i(y),
          \quad \text{with }
            \int_{\bbX} \phi_i(x) \phi_j(x) \mu(\diff x) = \delta_{ij}.
        \end{equation*}
        In the \href{https://dppy.readthedocs.io/en/latest/finite_dpps/exact_sampling.html}{\textcolor{magenta}{finite case}}, the eigendecomposition of the kernel matrix $\bfK$ can always be computed.
        \citet[Theorem 7]{HKPV06} proved that sampling $\DPP(K)$ can be done in two steps:
        \begin{enumerate}
            \item draw $B_i\sim\Ber(\lambda_i)$ independently and denote $\lrcb{i_1,\dots,i_{N}} = \lrcb{i:B_i=1}$,\label{enum:sampling_step1}
            \item sample from the DPP with kernel $\tilde{K}(x,y) = \sum_{n=1}^{N}\phi_{i_n}(x) \phi_{i_n}(y)$.\label{enum:sampling_step2}
        \end{enumerate}
        In other words, all DPPs are mixtures of \emph{projection} DPPs, that is, DPPs parametrized by an orthogonal projection kernel.
        In a nutshell, step \ref{enum:sampling_step1} selects a component of the mixture and step \ref{enum:sampling_step2} generates a sample from the corresponding projection DPP.
        \citet[Algorithm 18]{HKPV06} provide a generic way to sample from projection DPPs, which stems from two facts.
        First, \GG{conditionally on $N$ ?} a projection DPP with kernel $\tilde{K}$ is formed by exactly $N=\rank \tilde{K}$ points, $\mu$-almost surely.
        Then, one can apply the chain rule to sample $(X_1,\dots,X_N)$ with probability distribution given by \Eqref{eq:joint_distribution}, and discard the sequential aspect of the procedure to get a valid sample $\lrcb{X_{1}, \dots, X_{N}} \sim \DPP(\tilde{K})$.
        \begin{equation}
        \label{eq:joint_distribution}\\
        \!
        \frac{\det[\tilde K(x_m,x_n)]_{m,n=1}^N}{N!}
            \prod_{n=1}^N \mu(\diff x_n)
        % = \frac{1}{N!}
        %     \Vol^2\lrcb{\Phi(x_1),\dots,\Phi(x_n)}
        %         \prod_{n=1}^N \mu(\diff x_n)
            % \label{eq:joint_distribution1}\\
        =
        % \underbrace{
                \frac{\|\Phi(x_1)\|^2}{N}
            % }_{p(x_1)}
            \mu(\diff x_1)
            \prod_{n=2}^{N}
            % \underbrace{
                \frac{\| (I_N-\Pi_{H_{n-1}}) \Phi(x_n)\|^2}{N-(n-1)}
              % }_{p(x_n\vert x_1,\dots,x_{n-1})}
                \mu(\diff x_n),
        \!
        \end{equation}
        where
        $\Phi(x) \triangleq \lrp{\phi_{i_1}(x),\dots,\phi_{i_N}(x)}$
        denotes the \emph{feature vector} associated with $x\in\bbX$,
        so that
        $\tilde{K}(x,y) = \Phi(x)^{\top} \Phi(y)$
        and
        $\Pi_{H_{n-1}}$ the orthogonal projection matrix onto of
        $\Span\lrcb{\Phi(x_m)}_{m=1}^{n-1}$.

        % The algorithm boils down to expressing the chain rule for \Eqref{eq:joint_distribution1} as
        % % \begin{equation}
        % % \label{eq:joint_distribution2}
        % % \underbrace{
        % %   \frac{1}{N}
        % %     \lrnorm{\phi(x_1)}^2
        % %   }_{p(x_1)}
        % %       \mu(\diff x_1)
        % % \,\prod_{n=2}^{N}\,
        % % \underbrace{
        % %   \frac{1}{N-(n-1)}
        % %     \lrnorm{ \Pi_{H_{n-1}^{\perp}} \phi(x_n)}^2
        % %   }_{p(x_n\vert x_1,\dots,x_{n-1})}
        % %       \mu(\diff x_n),
        % % \end{equation}
        % where $\Pi_{H_{n-1}}$ is the orthogonal projection onto $H_{n-1} \triangleq \Span\lrcb{\Phi(x_1), \dots, \Phi(x_{n-1}) }$.
        % The crux of \citet[Algorithm 18]{HKPV06} is that since $\tilde K$ is a projection kernel, each term in \Eqref{eq:joint_distribution2} is a well-normalized conditional density, making \Eqref{eq:joint_distribution2} a chain rule.
        % It is then enough to sample from each conditional sequentially.
        % \GG{Insist on exchangeability/invariance by permutation}
        % \begin{equation}
        % \label{eq:conditionals_densities}
        %   p(x_1)
        %     = \frac{1}{N} \lrnorm{\phi(x_1)}^2
        %     \quad\text{and}\quad
        %   p(x_n | x_1,\dots,x_{n-1})
        %     = \frac{1}{N-(n-1)} \lrnorm{ \Pi_{H_{n-1}^{\perp}} \phi(x_n)}^2\!,
        % \end{equation}
        % \begin{align*}
        %   g_1(x)           &= \frac{1}{N} \lrnorm{\phi(x)}^2
        %                    &= \frac{1}{N} K(x,x) \\
        %   g_{n | 1:n-1}(x) &= \frac{1}{N-(n-1)}
        %                       \lrnorm{ \Pi_{H_{n-1}^{\perp}} \phi(x)}^2 \\
        %                    &= \frac{1}{N-(n-1)}
        %                       \lrb{K(x,x) - K(x,x_{1:n-1}) \lrb{K(x_k,x_l)}_{k,l=1}^{n-1} K(x_{1:n-1},x)}
        % \end{align*}

        A few remarks are in order.
        First, the LHS of \Eqref{eq:joint_distribution} is indeed an exchangeable probability distribution, the $N!$ factor captures all possible permutations.
        The successive ratios that appear in the RHS are the normalized conditional densities (w.r.t.\,$\mu$) which drive the chain rule.
        Observe that the respective normalizing constants are independent of the previous points.
        Second, the numerators of the RHS can be expressed as the ratio of two determinants and further expanded with Woodbury's formula.
        In that case, in each conditional, ML practitioners will recognize the incremental posterior variances in Gaussian process regression with kernel $\tilde K$ \citep[Equation 2.26]{RaWi06}.
        We insist that this link with Gaussian processes is a \href{https://dppy.readthedocs.io/en/latest/finite_dpps/exact_sampling.html#caution}{\textcolor{magenta}{consequence}} of $\tilde K$ being a \emph{projection} kernel.
        Third, the chain rule in \Eqref{eq:joint_distribution2} has a strong Gram-Schmidt flavor since it actually comes from a recursive application of the base$\times$height formula.
        In the end, DPPs favor configuration of points whose feature vectors $\Phi(x_1),\dots, \Phi(x_N)$ span a large volume, which is another way of understanding repulsiveness.

        The previous sampling scheme is exact and generic but requires the eigendecomposition of the underlying kernel.
        In the finite setting, this corresponds to an initial $\calO(M^3)$ cost, but then the procedure has an average cost of $\calO(M\lrb{\Tr \bfK}^2)$
        % which may still be problematic for large $M$,
        \citep{TrBaAm18}.
        In the continuous case, there is an additional cost of the rejection sampling routine required for sampling each conditional.
        In applications where these costs are a bottleneck, users rely on approximate sampling.
        Research has focused on two main directions: kernel approximation \citep{AKFT13} and MCMC samplers \citep{AnGhRe16, LiJeSr16c, GaBaVa17}.
        Mention \citep{Pou19,DeCaVa19}

        To conclude, some specific DPPs admit more efficient exact samplers that do not rely on \Eqref{eq:joint_distribution}, \eg uniform spanning trees \citep{PrWi98} or eigenvalues of random matrices.
        For instance, a $\beta$-ensemble is a set of $N$ points $\{X_1,\dots,X_N\}\subset \bbR$ with pdf
        \begin{equation*}
        \label{eq:beta_ensemble_pdf}
        \frac{1}{Z_{N,\beta}}
        \prod_{m< n}
            \lrabs{x_m-x_n}^{\beta}
        \prod_{n= 1}^N
            \omega(x_n)
            \diff x_n,
        \end{equation*}
        where $\beta>0$.
        For some choices of the weight function $\omega$, the $\beta$-ensemble can be sampled as the \href{https://dppy.readthedocs.io/en/latest/continuous_dpps/beta_ensembles.banded_models.html}{\textcolor{magenta}{spectrum of simple tridiagonal random matrices}} \citep{DuEd02,KiNe04}.
        When $\beta=2$, $\beta$-ensembles are projection DPPs \citep{Kon05}, and therefore examples of continuous DPPs that can be sampled in $\calO(N^2)$.
        Some of these ensembles are of direct interest to MLers \eg the \emph{Laguerre} ensemble for which $\omega$ is a Gamma pdf, is associated to the eigenvalues of the empirical covariance matrix of \iid Gaussian vectors.
        \GG{Mention Multivariate Jacobi ensemble}

    % section sampling (end)

% section determinantal_point_processes (end)

\section{The \DPPy\ toolbox} % (fold)
\label{sec:the_dppy_toolbox}

    \lstset{language=mypython}

    \DPPy\ handles objects that fit the natural definition of the different DPPs models.
    \begin{itemize}
        \item The \DPPy\ object corresponding to the finite $\DPP(\bfK)$ can be instantiated as
        \begin{lstlisting}[aboveskip=5pt,
                      belowskip=-1pt,
                      xleftmargin=.1\textwidth,
                      xrightmargin=.2\textwidth]
        FiniteDPP(kernel_type="correlation", **{"K": K}).
        \end{lstlisting}
        It has two main sampling methods, namely
        \lstinline{.sample_exact()} and
        \lstinline{.sample_mcmc()},
        implementing different variants of the exact sampling scheme and current state-of-the-art MCMC samplers.\\
        An additional
        \lstinline{.sample_exact_k_dpp()}
        allows to sample $k$-$\DPP$s \ie $\DPP$s conditionned to fixed cardinality samples $|\calX|=k$ \citep[Section 5]{KuTa12}.

        \item The \DPPy\ object associated to, \eg the Laguerre $\beta$-ensemble can be instantiated as
        \begin{lstlisting}[aboveskip=5pt,
                          belowskip=-1pt,
                          xleftmargin=.2\textwidth,
                          xrightmargin=.2\textwidth]
          LaguerreEnsemble(beta=3.14).
        \end{lstlisting}
        It can be sampled using either the full matrix model (eigenvalues of random covariance matrix) when $\beta\in\lrcb{1,2,4}$ or the tridiagonal one for $\beta > 0$,
          \begin{lstlisting}[aboveskip=5pt,
                          belowskip=-1pt,
                          xleftmargin=.1\textwidth,
                          xrightmargin=.2\textwidth]
          .sample_banded_model(shape=1.0, scale=2.0, size=50).
          \end{lstlisting}
        Displays are available via
        \lstinline{.plot()} to plot the last realization and
        \lstinline{.hist()} to construct the empirical distribution that converges to the Mar\v{c}enko-Pastur distribution.
    \end{itemize}
    More information can be found in the documentation\footnoteref{fn:docs} and the corresponding Jupyter \href{https://github.com/guilgautier/DPPy/tree/master/notebooks}{\textcolor{magenta}{notebooks}}, which showcase \DPPy\ objects.

% section the_dppy_toolbox (end)

\section{Conclusion and future work} % (fold)
\label{sec:conclusion_and_future_work}

    \DPPy\ can readily serve as research and teaching material.
    \DPPy\ is also ready for other contributors to add content and enlarge its scope, \eg with procedures for learning kernels.

% section conclusion_and_future_work (end)

% Acknowledgements should go at the end, before appendices and references
%\newpage
%{\small
\newpage

\acks{%
We would like to thank \href{https://guillep.github.io/}{Guillermo Polito} for leading our reproducible research \href{https://github.com/CRIStAL-PADR/reproducible-research-SE-notes}{workgroup}, this project owes him a lot.
We acknowledge funding by European CHIST-ERA project DELTA, the French Ministry of Higher Education and Research, the Nord-Pas-de-Calais Regional Council, Inria and Otto-von-Guericke-Universit\"at Magdeburg associated-team north-European project Allocate, and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003).
}

% Manual newpage inserted to improve the layout of sample file - not
% needed in general before the appendices/bibliography.

% \newpage

% \appendix
% \section*{Appendix A.}
% \label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

% \vskip 0.2in

\bibliography{biblio_new}

\end{document}
